{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VLM Embedding Stitching Benchmark\n",
        "\n",
        "**Goal**: Compare how fast three frozen vision encoders (ViT, CLIP, I-JEPA) converge when stitched into a Qwen-0.5B LLM via a trainable MLP projector + LoRA.\n",
        "\n",
        "| Encoder | Model | Hidden Dim | Patches |\n",
        "|---------|-------|-----------|----------|\n",
        "| ViT-L/16 | `google/vit-large-patch16-224` | 1024 | 196 |\n",
        "| CLIP ViT-L/14 | `openai/clip-vit-large-patch14` | 1024 | 256 |\n",
        "| I-JEPA ViT-H/14 | `facebook/ijepa_vith14_1k` | 1280 | 256 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies (uncomment on Colab)\n",
        "!pip install -q torch transformers peft datasets accelerate Pillow matplotlib tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: /Users/tarun/Personal/RLJ\n",
            "PyTorch:      2.10.0\n",
            "CUDA:         False (N/A)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "# If running on Colab, clone the repo first:\n",
        "# !git clone <your-repo-url> RLJ\n",
        "# %cd RLJ\n",
        "\n",
        "# Make sure project root is on the path\n",
        "PROJECT_ROOT = os.path.abspath(\".\")\n",
        "if PROJECT_ROOT not in sys.path:\n",
        "    sys.path.insert(0, PROJECT_ROOT)\n",
        "\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n",
        "print(f\"PyTorch:      {torch.__version__}\")\n",
        "print(f\"CUDA:         {torch.cuda.is_available()} ({torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Hardware: MPS/Mac\n",
            "  vit    | google/vit-large-patch16-224 | bs=2 | float32\n",
            "  clip   | openai/clip-vit-large-patch14 | bs=2 | float32\n",
            "  ijepa  | facebook/ijepa_vith14_1k | bs=2 | float32\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from configs import ExperimentConfig, get_encoder_configs\n",
        "\n",
        "# Get pre-built configs for all three encoders\n",
        "all_configs = get_encoder_configs()\n",
        "\n",
        "# ---- Detect hardware ----\n",
        "IS_COLAB = torch.cuda.is_available()\n",
        "IS_MAC = (not IS_COLAB) and hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
        "\n",
        "# ---- Adjust hyperparams for your hardware ----\n",
        "for cfg in all_configs:\n",
        "    cfg.num_steps = 500            # increase for a more thorough run\n",
        "    cfg.learning_rate = 1e-4\n",
        "    cfg.max_samples = 10000        # use a subset for speed (None = all)\n",
        "    cfg.log_every = 10\n",
        "    cfg.eval_every = 50\n",
        "    cfg.save_dir = \"outputs\"\n",
        "\n",
        "    if IS_COLAB:\n",
        "        cfg.batch_size = 4\n",
        "        cfg.gradient_accumulation_steps = 4\n",
        "        cfg.dtype = \"bfloat16\"\n",
        "    else:\n",
        "        # Mac / CPU -- smaller batches, float32 (bfloat16 crashes MPS)\n",
        "        cfg.batch_size = 2\n",
        "        cfg.gradient_accumulation_steps = 8\n",
        "        cfg.dtype = \"float32\"\n",
        "\n",
        "# Quick overview\n",
        "hw = \"CUDA/Colab\" if IS_COLAB else (\"MPS/Mac\" if IS_MAC else \"CPU\")\n",
        "print(f\"  Hardware: {hw}\")\n",
        "for c in all_configs:\n",
        "    print(f\"  {c.encoder_name:6s} | {c.encoder_model_id} | bs={c.batch_size} | {c.dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Run All Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from train import run_all_experiments\n",
        "\n",
        "trackers = run_all_experiments(all_configs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3b. (Alternative) Run a Single Encoder\n",
        "\n",
        "Uncomment and run this cell instead of cell 3 if you want to run one encoder at a time (useful for limited GPU memory)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [INFO] MPS detected -- forcing float32 (bfloat16 is unstable on MPS)\n",
            "======================================================================\n",
            "  EXPERIMENT: CLIP\n",
            "  Encoder:    openai/clip-vit-large-patch14\n",
            "  LLM:        Qwen/Qwen2.5-0.5B-Instruct\n",
            "  Device:     mps   Dtype: torch.float32\n",
            "======================================================================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eacf7923bb03459f8be0e1dcfcc7593c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1mCLIPVisionModel LOAD REPORT\u001b[0m from: openai/clip-vit-large-patch14\n",
            "Key                                                          | Status     |  | \n",
            "-------------------------------------------------------------+------------+--+-\n",
            "text_model.encoder.layers.{0...11}.self_attn.out_proj.weight | UNEXPECTED |  | \n",
            "text_model.encoder.layers.{0...11}.self_attn.k_proj.bias     | UNEXPECTED |  | \n",
            "text_model.encoder.layers.{0...11}.mlp.fc1.weight            | UNEXPECTED |  | \n",
            "text_model.embeddings.position_ids                           | UNEXPECTED |  | \n",
            "text_model.encoder.layers.{0...11}.self_attn.v_proj.weight   | UNEXPECTED |  | \n",
            "text_model.encoder.layers.{0...11}.layer_norm2.weight        | UNEXPECTED |  | \n",
            "text_model.encoder.layers.{0...11}.self_attn.q_proj.bias     | UNEXPECTED |  | \n",
            "text_model.encoder.layers.{0...11}.self_attn.q_proj.weight   | UNEXPECTED |  | \n",
            "text_model.encoder.layers.{0...11}.mlp.fc2.bias              | UNEXPECTED |  | \n",
            "text_model.encoder.layers.{0...11}.self_attn.v_proj.bias     | UNEXPECTED |  | \n",
            "text_model.encoder.layers.{0...11}.layer_norm1.bias          | UNEXPECTED |  | \n",
            "text_model.encoder.layers.{0...11}.layer_norm1.weight        | UNEXPECTED |  | \n",
            "text_model.encoder.layers.{0...11}.layer_norm2.bias          | UNEXPECTED |  | \n",
            "text_model.embeddings.token_embedding.weight                 | UNEXPECTED |  | \n",
            "text_model.encoder.layers.{0...11}.self_attn.out_proj.bias   | UNEXPECTED |  | \n",
            "text_model.encoder.layers.{0...11}.mlp.fc1.bias              | UNEXPECTED |  | \n",
            "text_model.encoder.layers.{0...11}.mlp.fc2.weight            | UNEXPECTED |  | \n",
            "text_model.encoder.layers.{0...11}.self_attn.k_proj.weight   | UNEXPECTED |  | \n",
            "vision_model.embeddings.position_ids                         | UNEXPECTED |  | \n",
            "text_model.final_layer_norm.bias                             | UNEXPECTED |  | \n",
            "text_model.embeddings.position_embedding.weight              | UNEXPECTED |  | \n",
            "text_model.final_layer_norm.weight                           | UNEXPECTED |  | \n",
            "logit_scale                                                  | UNEXPECTED |  | \n",
            "text_projection.weight                                       | UNEXPECTED |  | \n",
            "visual_projection.weight                                     | UNEXPECTED |  | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9bcd287334c84321947f980ca13de90e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/290 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Trainable params: 2,805,504  (0.35%)\n",
            "  Total params:     800,018,048\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7833e7e14a884e028a6d369e727f78ec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "CLIP:   0%|          | 0/500 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from train import run_experiment\n",
        "\n",
        "# Pick which encoder to run: 0 = ViT, 1 = CLIP, 2 = I-JEPA\n",
        "encoder_idx = 1  # CLIP\n",
        "cfg = all_configs[encoder_idx]\n",
        "\n",
        "tracker = run_experiment(cfg)\n",
        "print(tracker.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Plot Convergence Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils import plot_convergence\n",
        "\n",
        "fig = plot_convergence(\n",
        "    trackers,\n",
        "    title=\"Convergence: ViT vs CLIP vs I-JEPA\",\n",
        "    smoothing_window=20,\n",
        "    save_path=\"outputs/convergence.png\",\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Summary & Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "results = {}\n",
        "for name, tracker in trackers.items():\n",
        "    s = tracker.summary()\n",
        "    results[name] = s\n",
        "    print(f\"{name.upper():8s} | final_loss={s['final_loss']:.4f}  \"\n",
        "          f\"min_loss={s['min_loss']:.4f}  \"\n",
        "          f\"avg_last_50={s['avg_loss_last_50']:.4f}\")\n",
        "\n",
        "# Save summary\n",
        "os.makedirs(\"outputs\", exist_ok=True)\n",
        "with open(\"outputs/summary.json\", \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "print(\"\\nSaved to outputs/summary.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. (Optional) Load Previous Results\n",
        "\n",
        "If you ran experiments separately, you can reload the saved trackers and re-plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from utils import LossTracker, plot_convergence\n",
        "#\n",
        "# reloaded = {\n",
        "#     \"vit\":   LossTracker.load(\"outputs/vit/loss_history.json\"),\n",
        "#     \"clip\":  LossTracker.load(\"outputs/clip/loss_history.json\"),\n",
        "#     \"ijepa\": LossTracker.load(\"outputs/ijepa/loss_history.json\"),\n",
        "# }\n",
        "# fig = plot_convergence(reloaded)\n",
        "# fig.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
